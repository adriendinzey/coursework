---
title: "Assignment 1 3550"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
```

## R Markdown
2.9 a)
```{r}
carData<-data.frame(x1=c(14.620,15.630,14.620,15.000,14.500,15.250,16.120,15.130,15.500,15.130,15.500,16.120,15.130,15.630,15.380,15.500,144.250,14.500,14.620),x2=c(226.00,220.00,217.40,220.00,226.50,224.10,220.50,223.50,217.60,228.50,230.20,226.50,226.60,225.60,234.00,230.00,224.30,240.50,223.70),x3=c(7.000,3.375,6.375,6.000,7.625,6.000,3.375,6.125,5.000,6.625,5.750,3.750,6.125,5.375,8.875,4.000,8.000,10.870,7.375),y=c(128.40,52.62,113.90,98.01,139.90,102.60,48.14,109.60,82.68,112.60,97.52,59.06,111.80,89.09,171.90,66.80,157.10,208.40,133.40))
mod<-lm(y~x1+x2+x3,data=carData)
summary(mod)
```

2.9 b)
```{r}
params<-mod$coefficients
hFE<-params[1]+14.5*params[2]+220*params[3]+5.0*params[4]
hFE
```

2.9 c)
```{r}
summary(aov(mod))
```
As you can see at the alpha level 0.05 we reject the null hypothesis
2.9 d)
this is just the MSE (i.e. mean of the squared residuals)
```{r}
mean(mod$residuals^2)
```

2.9 e)
```{r}
sqrt(diag(vcov(mod)))
```

2.9 f)
if we look at the t Value column we get the t-values for each statistic. The next column are the p-values for the t-test and at a 0.05 significance level we would reject the null hypothesis for x1 and x2 while we would fail to reject for x3.
```{r}
summary(mod)
```

2.9 g)
```{r}
confint(mod, level =0.99)
```

2.9 h)
```{r}
predictionData<-data.frame(x1=14.5,x2=220,x3=5.0)
predict(mod, predictionData, interval="predict",level = 0.99)
```
The prediction interval is (72.30447,93.77319)

2.9 i)
```{r}
predictionData<-data.frame(x1=14.5,x2=220,x3=5.0)
predict(mod, predictionData, interval="confidence",level = 0.99)
```
The mean response confidence interval is (79.63722,86.44044)


2.15)
```{r}
data1<-data.frame(y=c(1275,1350,1650,2000,3750,4222,5018,6125,6200,8150,9975,12200,12750,13014,13275),x=c(230,235,250,277,522,545,625,713,735,820,992,1322,1900,2022,2155))
```



2.15 a)
using the first data values to form an estimate:
```{r}
nls1<-nls(y~a+b*x,data1,start = list(a=100,b=5))
summary(nls1)
plot(data1$x,data1$y)
lines(data1$x,predict(nls1),col="blue")
```

2.15 b)
using the first data values to form an estimate:
```{r}
nls1<-nls(y~exp(a+b*(1/x)),data1,start = list(a=7,b=230))
summary(nls1)
plot(data1$x,data1$y)
lines(data1$x,predict(nls1),col="blue")
```

2.15 c)
using the first data values to form an estimate:
```{r}
nls1<-nls(y~1/(a+b*(1/x)),data1,start = list(a=.0004,b=.00230))
summary(nls1)
plot(data1$x,data1$y)
lines(data1$x,predict(nls1),col="blue")
```

2.15 d)
using the first data values to form an estimate:
```{r}
nls1<-nls(y~a+b*x+c*(x^2),data1,start = list(a=50,b=3,c=.01))
summary(nls1)
plot(data1$x,data1$y)
lines(data1$x,predict(nls1),col="blue")
```

2.15 e)
For a) we see that the p-value for b is very low however the p-value for a is quite high in comparison. If we look at the plot the points seem to not fit the line at all.

For b) we see that both paramters have a very very low p-value which indicates they are most likely very accurate. The plot seems to also fit the shape of the data well.

For c) the second parameter seems to be pretty accurate and the first parameter's p-value would likely pass most hypothesis tests however it does not have the same accuracy as b). Looking at the plot we can see it doesn't curve as well as b) does.

for d) the second and third parameters have low p-values so they are likely strongly accurate and the first parameter also has a decently low value meaning this model has overall good accuracy. Looking at c) and d) we can see that the more accurate a model gets, the closer the plot looks like the plot in b). This indicates, along with the p-values of the parameters, that b) is likely the most accurate model.

```{r}
data2<- data.frame(y=c(2.240,3.581,5.131,5.715,0.889,2.845,6.147,7.016,7.747,8.286,9.321,9.195,1.895,3.708,3.467,5.049,4.846,6.108,7.137,8.295,1.168,2.865,3.912,3.726,4.521,5.664,7.620,9.766,12.497,2.174,8.153,10.757),x1=c(24.00,43.00,84.00,159.00,3.79,17.00,37.00,54.20,58.00,85.00,89.00,95.00,1.85,5.88,7.02,13.88,17.19,19.80,33.80,40.80,0.83,8.08,21.08,21.08,7.00,15.00,27.00,52.00,65.00,1.76,7.90,20.90),x2=c(1000,1000,1000,1000,1100,1100,1100,1100,1100,1100,1100,1100,1420,1420,1420,1420,1420,1420,1420,1420,1825,1825,1825,1825,1400,1400,1400,1400,1400,1310,1310,1310),x3=c(8.47,8.55,8.53,8.60,7.72,7.83,4.91,3.79,3.69,3.70,3.91,3.57,4.43,3.67,2.89,2.56,2.56,2.84,3.22,3.54,3.73,3.45,2.85,2.85,8.88,8.88,8.88,8.88,8.88,5.46,2.15,2.55),x4=c(299,299,299,297,300,299,299,299,299,298,297,296,298,298,298,298,298,298,298,298,298,295,298,298,303,303,303,303,303,313,309,312))
```

2.16 a)
```{r}
mlrMod<-lm(y~x1+x2+x3,data=data2)
summary(mlrMod)
```
We can see from the p-values that most of these parameters would not be accepted in t tests by a large margin at any reasonable significance level. This hints that this model is not very adequate.

2.16 b)
```{r}
bc<-boxcox(mlrMod,lambda = seq(-3, 3, length = 10))
```
The max (lambda) is
```{r}
bc$x[which(bc$y==max(bc$y))]
qchisq(.95,1)
```
2.16 c)
We can easily see from the plot that the value lambda=1 is present in the 95% confidence interval so it is possible that this transformation is unnecessary.

2.16 d)
Transformations is likely not needed


3.3
```{r}
b1=10
b2=2
b3=c(.5,1,2,3)
x=seq(-10,10,by=.01)

y<-b1/(1+b2*exp(-b3[1]*x))
y2<-b1/(1+b2*exp(-b3[2]*x))
y3<-b1/(1+b2*exp(-b3[3]*x))
y4<-b1/(1+b2*exp(-b3[4]*x))

plot(x,y,col="yellow",lwd=1)
lines(x,y2,col="red",lwd=1)
lines(x,y3,col="blue",lwd=1)
lines(x,y4,col="green",lwd=1)
```
We can see that as b3 increases, the response variable y grows quicker and thus the slopes are steeper and the graph plateaus quicker.

3.4
```{r}
b1=1
b3=1
b2=c(1,4,8)
x=seq(-10,10,by=.01)

y<-b1/(1+b2[1]*exp(-b3*x))
y2<-b1/(1+b2[2]*exp(-b3*x))
y3<-b1/(1+b2[3]*exp(-b3*x))

plot(x,y,col="red",lwd=1)
lines(x,y2,col="blue",lwd=1)
lines(x,y3,col="green",lwd=1)
```
We can see that as b2 increases, the graph shifts forward. This means it increases each response by a constant value, but doesa not change the rate of growth

3.8
```{r}
x<-c(.5,1,2,4,8,9,10)
y1<-c(0.68,0.45,2.50,6.19,56.1,89.8,147.7)
y2<-c(1.58,2.66,2.04,7.85,54.2,90.2,146.3)
y<-(y1+y2)/2
df<-data.frame(x,y)
```

a)
I will take a point near the middle and calculate a b1 and b2 that works for that point

b)


```{r}
nlsModel<-nls(y~a*exp(b*x),df,start = list(a=0.7,b=.25))
summary(nlsModel)
plot(df$x,df$y)
lines(df$x,predict(nlsModel),col="blue")
```
c)
We can easily see from the p-values that both parameter estimates have tiny tiny p-values. This indicates that we should reject the null hypothesis that both of them are 0 as at least 1 of them is likely not 0.

d)
We will use the Residual standard error squared as RSE is an estimate for sigma
```{r}
sigma(nlsModel)^2
```
e)
We see that the p-values for the parameters are tiny, less than any reasonable significance level so we can say we reject the null hypothesis and conclude that the two parameters are different from 0.

f)

```{r}
yhat = fitted(nlsModel)
e = residuals(nlsModel)
qqnorm(e)
qqline(e)
plot(yhat, e)
```

We can see from the plot that residuals seem to be randomly distributed and from the qq plot that there seems to be a linear relationship between sample and theoretical quantiles with a slope of 1. These both indicate that the sample quantiles are likely from the same distribution as the theoretical quantiles.


3.11
```{r}
y1<-c(0.49,0.48,0.46,0.45,0.44,0.46,0.42,0.41,0.42,0.41,0.41,0.40,0.41,0.40,0.41,0.40,0.39,0.39)
y2<-c(0.49,0.47,0.46,0.43,0.43,0.45,0.42,0.41,0.4,0.4,0.4,0.4,0.4,0.4,0.38,0.4,0.39,0.39)
y3<-c(0.490,0.480,0.450,0.430,0.430,0.455,0.430,0.400,0.400,0.410,0.405,0.380,0.405,0.400,0.395,0.400,0.390,0.390)
y4<-c(0.49,0.77,0.43,0.43667,0.43333,0.45500,0.42333,0.40667,0.40667,0.40667,0.40500,0.39333,0.40500,0.40000,0.39500,0.40000,0.39000,0.39000)
y<-(y1+y2+y3+y4)/4
x<-c(8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42)
df<-data.frame(x,y)
```
a)
```{r}

plot(x,y)
```
b)
using x=22 and y=0.4066 we can estimate the parameters because the 22nd root of 0.4066 is about 0.96 so plugging in .959^22 and then fixing that value to equal around .6 by multiplying by b and then doing 1 minus that value we get
```{r}
mitchModel<-nls(y~a-b*(k^x),df,start=list(a=1,b=1.5,k=0.959))
summary(mitchModel)
```

c)
From the p-values of the parameters we can see that at least one is very very tiny and so we can conclude that not all parameters are 0. This means we reject the null hypothesis for the significance of regression test.

d)
```{r}
confint(mitchModel)
```

From these intervals we can see that it is likely all the model parameters differ from 0 as the value 0 is not contained in any of the intervals.

e)
The estimate for sigma is residual standard error so we will simply square it
```{r}
sigma(nlsModel)^2
```